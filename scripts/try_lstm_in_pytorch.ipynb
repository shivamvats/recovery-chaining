{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ef2386-9880-4949-aa22-897d12886082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fafbda807f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e0148c-4917-4a95-8aef-98f2c00ae856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(3, 3, batch_first=True)\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[0.1896, 1.4122, 0.9275]])\n",
      "  h0, c0: (tensor([[[-0.5745, -0.5534, -0.5951]]]), tensor([[[-0.4152,  0.0140,  0.3971]]]))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.3102, -0.0439,  0.2343]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.3102, -0.0439,  0.2343]]], grad_fn=<StackBackward0>), tensor([[[-0.4907, -0.2764,  0.2900]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[ 0.3314, -0.3171,  0.6291]])\n",
      "  h0, c0: (tensor([[[-0.3102, -0.0439,  0.2343]]], grad_fn=<StackBackward0>), tensor([[[-0.4907, -0.2764,  0.2900]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.1795, -0.0766,  0.2227]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.1795, -0.0766,  0.2227]]], grad_fn=<StackBackward0>), tensor([[[-0.4191, -0.2284,  0.3128]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[-2.0567,  0.6842, -0.5331]])\n",
      "  h0, c0: (tensor([[[-0.1795, -0.0766,  0.2227]]], grad_fn=<StackBackward0>), tensor([[[-0.4191, -0.2284,  0.3128]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.2321, -0.0814,  0.0357]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.2321, -0.0814,  0.0357]]], grad_fn=<StackBackward0>), tensor([[[-0.4535, -0.3321,  0.1033]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[-0.1527, -0.6993,  0.4899]])\n",
      "  h0, c0: (tensor([[[-0.2321, -0.0814,  0.0357]]], grad_fn=<StackBackward0>), tensor([[[-0.4535, -0.3321,  0.1033]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.1395, -0.0706,  0.0953]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.1395, -0.0706,  0.0953]]], grad_fn=<StackBackward0>), tensor([[[-0.3280, -0.2029,  0.1595]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[0.3985, 0.7510, 0.2175]])\n",
      "  h0, c0: (tensor([[[-0.1395, -0.0706,  0.0953]]], grad_fn=<StackBackward0>), tensor([[[-0.3280, -0.2029,  0.1595]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.3046, -0.0742,  0.1729]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.3046, -0.0742,  0.1729]]], grad_fn=<StackBackward0>), tensor([[[-0.5378, -0.3068,  0.2376]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[0.7293, 0.8849, 0.2005]])\n",
      "  h0, c0: (tensor([[[-0.3046, -0.0742,  0.1729]]], grad_fn=<StackBackward0>), tensor([[[-0.5378, -0.3068,  0.2376]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.3537, -0.0721,  0.2369]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.3537, -0.0721,  0.2369]]], grad_fn=<StackBackward0>), tensor([[[-0.6513, -0.3135,  0.3047]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[ 0.1521, -0.2496,  2.0107]])\n",
      "  h0, c0: (tensor([[[-0.3537, -0.0721,  0.2369]]], grad_fn=<StackBackward0>), tensor([[[-0.6513, -0.3135,  0.3047]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.0782, -0.1315,  0.3307]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.0782, -0.1315,  0.3307]]], grad_fn=<StackBackward0>), tensor([[[-0.2088, -0.3640,  0.4082]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[ 0.8637,  0.6099, -1.1666]])\n",
      "  h0, c0: (tensor([[[-0.0782, -0.1315,  0.3307]]], grad_fn=<StackBackward0>), tensor([[[-0.2088, -0.3640,  0.4082]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.4015, -0.0672,  0.0797]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.4015, -0.0672,  0.0797]]], grad_fn=<StackBackward0>), tensor([[[-0.7032, -0.2748,  0.1272]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[1.1124, 0.3314, 2.9973]])\n",
      "  h0, c0: (tensor([[[-0.4015, -0.0672,  0.0797]]], grad_fn=<StackBackward0>), tensor([[[-0.7032, -0.2748,  0.1272]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.0717, -0.1344,  0.4458]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.0717, -0.1344,  0.4458]]], grad_fn=<StackBackward0>), tensor([[[-0.1658, -0.4257,  0.5070]]], grad_fn=<StackBackward0>))\n",
      "\n",
      "\n",
      "Input:\n",
      "  x: tensor([[-0.2197, -0.6007, -0.4284]])\n",
      "  h0, c0: (tensor([[[-0.0717, -0.1344,  0.4458]]], grad_fn=<StackBackward0>), tensor([[[-0.1658, -0.4257,  0.5070]]], grad_fn=<StackBackward0>))\n",
      "Output:\n",
      "  h1...ht: tensor([[[-0.1685, -0.1026,  0.0948]]], grad_fn=<TransposeBackward0>)\n",
      "  hT, cT: (tensor([[[-0.1685, -0.1026,  0.0948]]], grad_fn=<StackBackward0>), tensor([[[-0.3897, -0.2772,  0.1929]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 10, 3), got [1, 1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0436ce2ea47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clean out hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/python3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/python3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    729\u001b[0m                            ):\n\u001b[1;32m    730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    732\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    733\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/python3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    237\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_weights_have_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 10, 3), got [1, 1, 3]"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(input_size=3, hidden_size=3, num_layers=1, batch_first=True)  # Input dim is 3, hidden dim is 3\n",
    "print(lstm)\n",
    "seq_len = 3\n",
    "batch_size = 2\n",
    "inputs = [torch.randn(1, seq_len) for _ in range(batch_size)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "# (h0, c0)\n",
    "hidden = (torch.randn(1, 1, seq_len),\n",
    "          torch.randn(1, 1, seq_len))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    print(\"\\n\")\n",
    "    print(\"Input:\")\n",
    "    print(\"  x:\", i)\n",
    "    print(\"  h0, c0:\", hidden)\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(\"Output:\")\n",
    "    print(\"  h1...ht:\", out)\n",
    "    print(\"  hT, cT:\", hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa1b32-7e33-4d8e-aba8-bd3ea235980d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
